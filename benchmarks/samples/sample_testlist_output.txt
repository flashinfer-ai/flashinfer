$ python3 flashinfer_benchmark.py --testlist samples/sample_testlist.txt --output_path sample_testlist_output.csv
[INFO] args = Namespace(routine='BatchPrefillWithPagedKVCacheWrapper', no_cuda_graph=True, refcheck=True, allow_output_mismatch=False, random_seed=42, verbose=2, output_path=None, num_iters=30, dry_run_iters=5, backends=['fa2', 'trtllm-gen', 'cudnn'], page_size=16, batch_size=16, s_qo=1024, s_kv=1024, num_qo_heads=8, num_kv_heads=8, head_dim_qk=128, head_dim_vo=128, head_dim_ckv=None, head_dim_kpe=None, q_dtype='bfloat16', kv_dtype='bfloat16', causal=True, random_actual_seq_len=True)
[INFO] Running testBatchPrefillWithPagedKVCacheWrapper
[INFO] FlashInfer version: 0.2.8
[VVERBOSE] gpu_name = 'NVIDIA_B200'
[VERBOSE] Average actual seq len: 327
[VVERBOSE] actual_seq_lens_q.flatten() = tensor([103, 436, 861, 271, 107,  72, 701,  21, 615, 122, 467, 215, 331, 459,
         88, 373], dtype=torch.int32)
[VVERBOSE] q.shape = torch.Size([5242, 8, 128])
[VVERBOSE] num_pages_per_seq = 64
[VVERBOSE] total_num_pages = 1024
[VVERBOSE] kv_cache.shape = torch.Size([1024, 2, 8, 16, 128])
[VVERBOSE] kv_cache.stride() = (32768, 16384, 128, 1024, 1)
[VVERBOSE] block_tables.shape = torch.Size([16, 64])
[VVERBOSE] qo_indptr.shape = torch.Size([17])
[VVERBOSE] qo_indptr.dtype = torch.int32
[VVERBOSE] kv_indptr.shape = torch.Size([17])
[VVERBOSE] kv_indices.shape = torch.Size([335])
[VVERBOSE] kv_last_page_len.shape = torch.Size([16])
[VVERBOSE] scale = 0.08838834764831843
[PERF] fa2       :: median time 0.059 ms; std 0.001 ms; achieved tflops 91.327 TFLOPs/sec; achieved tb_per_sec 0.723 TB/sec
[PERF] trtllm-gen:: median time 0.053 ms; std 0.001 ms; achieved tflops 102.018 TFLOPs/sec; achieved tb_per_sec 0.808 TB/sec
[PERF] cudnn     :: median time 0.046 ms; std 0.001 ms; achieved tflops 117.710 TFLOPs/sec; achieved tb_per_sec 0.932 TB/sec
[INFO] args = Namespace(routine='BatchPrefillWithRaggedKVCacheWrapper', no_cuda_graph=True, refcheck=True, allow_output_mismatch=False, random_seed=42, verbose=2, output_path=None, num_iters=30, dry_run_iters=5, backends=['fa2', 'cudnn', 'trtllm-gen', 'cutlass'], page_size=0, batch_size=16, s_qo=1024, s_kv=1024, num_qo_heads=128, num_kv_heads=128, head_dim_qk=192, head_dim_vo=128, head_dim_ckv=None, head_dim_kpe=None, q_dtype='bfloat16', kv_dtype='bfloat16', causal=True, random_actual_seq_len=False)
[INFO] Running testBatchPrefillWithRaggedKVCacheWrapper
[INFO] FlashInfer version: 0.2.8
[VVERBOSE] gpu_name = 'NVIDIA_B200'
[INFO] trtllm-gen backend does not support ragged prefill. Skipping.
[VERBOSE] Average actual seq len: 1024
[VVERBOSE] actual_seq_lens_q.flatten() = tensor([1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,
        1024, 1024, 1024, 1024], dtype=torch.int32)
[VVERBOSE] q.shape = torch.Size([16384, 128, 192])
[VVERBOSE] k.shape = torch.Size([16384, 128, 192])
[VVERBOSE] v.shape = torch.Size([16384, 128, 128])
[VVERBOSE] qo_indptr.shape = torch.Size([17])
[VVERBOSE] kv_indptr.shape = torch.Size([17])
[VVERBOSE] scale = 0.07216878364870323
[PERF] fa2       :: median time 2.263 ms; std 0.032 ms; achieved tflops 303.712 TFLOPs/sec; achieved tb_per_sec 1.186 TB/sec
[PERF] cudnn     :: median time 1.076 ms; std 0.052 ms; achieved tflops 638.828 TFLOPs/sec; achieved tb_per_sec 2.495 TB/sec
[PERF] cutlass   :: median time 1.407 ms; std 0.019 ms; achieved tflops 488.420 TFLOPs/sec; achieved tb_per_sec 1.908 TB/sec
[INFO] args = Namespace(routine='BatchDecodeWithPagedKVCacheWrapper', no_cuda_graph=False, refcheck=True, allow_output_mismatch=False, random_seed=42, verbose=2, output_path=None, num_iters=30, dry_run_iters=5, backends=['fa2', 'fa2_tc', 'trtllm-gen', 'cudnn'], page_size=16, batch_size=16, s_qo=1, s_kv=1024, num_qo_heads=64, num_kv_heads=8, head_dim_qk=128, head_dim_vo=128, head_dim_ckv=None, head_dim_kpe=None, q_dtype='bfloat16', kv_dtype='bfloat16', causal=False, random_actual_seq_len=True)
[INFO] Running testBatchDecodeWithPagedKVCacheWrapper
[INFO] FlashInfer version: 0.2.8
[VVERBOSE] gpu_name = 'NVIDIA_B200'
[VERBOSE] Average actual seq len: 501
[VVERBOSE] actual_seq_lens_kv.flatten() = tensor([ 84, 874, 167, 691, 274, 736,  63, 813, 781, 450, 794, 226, 510, 499,
        524, 541], device='cuda:0', dtype=torch.int32)
[VVERBOSE] q.shape = torch.Size([16, 64, 128])
[VVERBOSE] num_pages_per_seq = 64
[VVERBOSE] total_num_pages = 1024
[VVERBOSE] kv_cache.shape = torch.Size([1024, 2, 8, 16, 128])
[VVERBOSE] kv_cache.stride() = (32768, 16384, 128, 1024, 1)
[VVERBOSE] block_tables.shape = torch.Size([16, 64])
[VVERBOSE] kv_indptr.shape = torch.Size([17])
[VVERBOSE] kv_indices.shape = torch.Size([509])
[VVERBOSE] kv_last_page_len.shape = torch.Size([16])
[VVERBOSE] scale = 0.08838834764831843
[PERF] fa2       :: median time 0.051 ms; std 0.000 ms; achieved tflops 5.193 TFLOPs/sec; achieved tb_per_sec 0.659 TB/sec
[PERF] fa2_tc    :: median time 0.016 ms; std 0.000 ms; achieved tflops 16.336 TFLOPs/sec; achieved tb_per_sec 2.075 TB/sec
[PERF] trtllm-gen:: median time 0.012 ms; std 0.000 ms; achieved tflops 22.529 TFLOPs/sec; achieved tb_per_sec 2.861 TB/sec
[PERF] cudnn     :: median time 0.012 ms; std 0.000 ms; achieved tflops 21.714 TFLOPs/sec; achieved tb_per_sec 2.757 TB/sec
[INFO] args = Namespace(routine='BatchMLAPagedAttentionWrapper', no_cuda_graph=True, refcheck=True, allow_output_mismatch=False, random_seed=42, verbose=2, output_path=None, num_iters=30, dry_run_iters=5, backends=['fa2'], page_size=16, batch_size=16, s_qo=1, s_kv=8192, num_qo_heads=128, num_kv_heads=128, head_dim_qk=None, head_dim_vo=None, head_dim_ckv=512, head_dim_kpe=64, q_dtype='bfloat16', kv_dtype='bfloat16', causal=True, random_actual_seq_len=True)
[INFO] Running testBatchMLAPagedAttentionWrapper
[INFO] FlashInfer version: 0.2.8
[VVERBOSE] gpu_name = 'NVIDIA_B200'
[VERBOSE] Average actual seq len: 3893
[VVERBOSE] actual_seq_lens_kv.flatten() = tensor([4180, 5994, 4263, 1715, 6418, 6880, 5183, 3885, 6925, 1474, 5914, 4322,
         510, 2547, 1548,  541], device='cuda:0', dtype=torch.int32)
[VVERBOSE] q_nope.shape = torch.Size([16, 128, 512])
[VVERBOSE] q_pe.shape = torch.Size([16, 128, 64])
[VVERBOSE] num_pages_per_seq = 512
[VVERBOSE] total_num_pages = 8192
[VVERBOSE] ckv_cache.shape = torch.Size([8192, 16, 512])
[VVERBOSE] kpe_cache.shape = torch.Size([8192, 16, 64])
[VVERBOSE] qo_indptr.shape = torch.Size([17])
[VVERBOSE] kv_indptr.shape = torch.Size([17])
[VVERBOSE] kv_indices.shape = torch.Size([3901])
[VVERBOSE] actual_seq_lens_kv.shape = torch.Size([16, 1, 1, 1])
[VVERBOSE] sm_scale = 0.041666666666666664
[VVERBOSE] workspace_buffer.shape = torch.Size([134217728])
[PERF] fa2       :: median time 0.098 ms; std 0.000 ms; achieved tflops 176.514 TFLOPs/sec; achieved tb_per_sec 1.581 TB/sec
[INFO] args = Namespace(routine='gemm_fp8_nt_groupwise', no_cuda_graph=True, refcheck=True, allow_output_mismatch=False, random_seed=42, verbose=2, output_path=None, num_iters=30, dry_run_iters=5, batch_size=1, m=8192, n=4096, k=16384, tile_size=128, group_size=1, scale_major_mode='MN', input_dtype='fp8_e4m3', mat2_dtype='fp8_e4m3', out_dtype='bfloat16', mma_sm=2, backends=['cudnn'])
[INFO] Running testGemmFp8NtGroupwise
[INFO] FlashInfer version: 0.2.8
[VVERBOSE] gpu_name = 'NVIDIA_B200'
[VVERBOSE] a_val.shape = torch.Size([8192, 16384])
[VVERBOSE] b_val.shape = torch.Size([4096, 16384])
[VVERBOSE] a_fp8.shape = torch.Size([8192, 16384])
[VVERBOSE] b_fp8.shape = torch.Size([4096, 16384])
[VVERBOSE] a_scale.shape = torch.Size([128, 8192])
[VVERBOSE] b_scale.shape = torch.Size([128, 32])
[PERF] gemm_fp8:: median time 0.661 ms; std 0.035 ms; achieved tflops 1662.340 TFLOPs/sec; achieved tb_per_sec 0.406 TB/sec
[INFO] args = Namespace(routine='group_gemm_fp8_nt_groupwise', no_cuda_graph=True, refcheck=True, allow_output_mismatch=False, random_seed=42, verbose=2, output_path=None, num_iters=30, dry_run_iters=5, batch_size=1, m=8192, n=4096, k=16384, tile_size=128, group_size=2, scale_major_mode='K', input_dtype='fp8_e4m3', mat2_dtype='fp8_e4m3', out_dtype='bfloat16', mma_sm=2, backends=['cudnn'])
[INFO] Running testGroupGemmFp8NtGroupwise
[INFO] FlashInfer version: 0.2.8
[VVERBOSE] gpu_name = 'NVIDIA_B200'
[VVERBOSE] a_val.shape = torch.Size([16384, 16384])
[VVERBOSE] b_val.shape = torch.Size([2, 4096, 16384])
[VVERBOSE] a_fp8.shape = torch.Size([16384, 16384])
[VVERBOSE] b_fp8.shape = torch.Size([2, 4096, 16384])
[VVERBOSE] a_scale.shape = torch.Size([16384, 128])
[VVERBOSE] b_scale.shape = torch.Size([2, 32, 128])
[VVERBOSE] m_indptr.shape = torch.Size([3])
[PERF] grp_gemm:: median time 1.458 ms; std 0.025 ms; achieved tflops 1508.760 TFLOPs/sec; achieved tb_per_sec 0.368 TB/sec
[INFO] args = Namespace(routine='group_gemm_fp8_nt_groupwise', no_cuda_graph=False, refcheck=True, allow_output_mismatch=False, random_seed=42, verbose=2, output_path=None, num_iters=30, dry_run_iters=5, batch_size=1, m=8192, n=4096, k=16384, tile_size=128, group_size=2, scale_major_mode='MN', input_dtype='fp8_e4m3', mat2_dtype='fp8_e4m3', out_dtype='bfloat16', mma_sm=2, backends=['cudnn'])
[INFO] Running testGroupGemmFp8NtGroupwise
[INFO] FlashInfer version: 0.2.8
[VVERBOSE] gpu_name = 'NVIDIA_B200'
[VVERBOSE] a_val.shape = torch.Size([16384, 16384])
[VVERBOSE] b_val.shape = torch.Size([2, 4096, 16384])
[VVERBOSE] a_fp8.shape = torch.Size([16384, 16384])
[VVERBOSE] b_fp8.shape = torch.Size([2, 4096, 16384])
[VVERBOSE] a_scale.shape = torch.Size([128, 16384])
[VVERBOSE] b_scale.shape = torch.Size([2, 128, 32])
[VVERBOSE] m_indptr.shape = torch.Size([3])
[PERF] grp_gemm:: median time 1.344 ms; std 0.083 ms; achieved tflops 1635.975 TFLOPs/sec; achieved tb_per_sec 0.399 TB/sec
[INFO] args = Namespace(routine='bmm_fp8', no_cuda_graph=False, refcheck=True, allow_output_mismatch=False, random_seed=42, verbose=2, output_path=None, num_iters=30, dry_run_iters=5, batch_size=1, m=8192, n=4096, k=16384, tile_size=128, group_size=1, scale_major_mode='MN', input_dtype='fp8_e4m3', mat2_dtype='fp8_e4m3', out_dtype='bfloat16', mma_sm=1, backends=['cudnn', 'cublas'])
[INFO] Running testBmmFp8
[INFO] FlashInfer version: 0.2.8
[VVERBOSE] gpu_name = 'NVIDIA_B200'
[VVERBOSE] input_fp8.shape = torch.Size([1, 8192, 16384])
[VVERBOSE] input_fp8.dtype = torch.float8_e4m3fn
[VVERBOSE] mat2_fp8.shape = torch.Size([1, 16384, 4096])
[VVERBOSE] mat2_fp8.dtype = torch.float8_e4m3fn
[VVERBOSE] input_inv_s = tensor(0.0127, device='cuda:0')
[VVERBOSE] input_inv_s.dtype = torch.float32
[VVERBOSE] mat2_inv_s = tensor(0.0127, device='cuda:0')
[VVERBOSE] mat2_inv_s.dtype = torch.float32
[PERF] cudnn     :: median time 0.444 ms; std 0.024 ms; achieved tflops 2478.409 TFLOPs/sec; achieved tb_per_sec 0.605 TB/sec
[PERF] cublas    :: median time 0.395 ms; std 0.032 ms; achieved tflops 2782.762 TFLOPs/sec; achieved tb_per_sec 0.679 TB/sec
