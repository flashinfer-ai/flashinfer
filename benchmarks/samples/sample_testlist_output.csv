routine,median_time,std_time,tflops,tb_per_sec,backend,s_qo,s_kv,head_dim_qk,head_dim_vo,head_dim_ckv,head_dim_kpe,causal,q_dtype,kv_dtype,avg_actual_seq_len,random_actual_seq_len,n,group_size,tile_size,scale_major_mode,mma_sm,use_128x4_sf_layout,use_nvfp4,num_tokens,intermediate_size,num_experts,top_k,n_group,topk_group,routed_scaling_factor,local_expert_offset,local_num_experts,routing_method,use_shuffled_weight,weight_layout,use_routing_bias,use_routing_scales_on_input,weight_dtype,gated_act,cutlass_variant,quantized_input,tp_size,tp_rank,ep_size,ep_rank,num_tokens,num_experts,top_k,ep_size,max_num_tokens,num_heads,scale,eps,use_global_scale,alignment,global_scale,sf_layout,do_shuffle,sf_vec_size,vocab_size,top_k,top_p,min_p,temperature,num_speculate_tokens,filter_apply_order,max_len,num_rows,seq_len,head_dim,rotary_dim,no_rope_dim,rope_theta,rope_scale,interleave,kv_layout,batch_size,hidden_size,input_dtype,out_dtype,quant_dtype,m,k,num_qo_heads,num_kv_heads,page_size,enable_pdl,is_sf_swizzled_layout,refcheck,no_cuda_graph,use_cupti,allow_output_mismatch,random_seed,case_tag,generate_repro_command,repro_command
BatchPrefillWithPagedKVCacheWrapper,0.011616,0.007307573782194897,14.963658402203857,0.32687603305785123,fa2,1024,1024,128,128,,,True,torch.bfloat16,torch.bfloat16,103,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,,,,,,64,8,16,,,True,False,True,True,42,Llama-3.1-70B,True,python3 flashinfer_benchmark.py --routine BatchPrefillWithPagedKVCacheWrapper --backends fa2 fa3 cudnn trtllm-gen --page_size 16 --batch_size 1 --s_qo 1024 --s_kv 1024 --num_qo_heads 64 --num_kv_heads 8 --head_dim_qk 128 --head_dim_vo 128 --random_actual_seq_len -vv --refcheck --causal --q_dtype bfloat16 --kv_dtype bfloat16 --allow_output_mismatch --generate_repro_command --case_tag Llama-3.1-70B
BatchPrefillWithPagedKVCacheWrapper,0.020448,0.0007820994992113643,8.50048200312989,0.1856901408450704,cudnn,1024,1024,128,128,,,True,torch.bfloat16,torch.bfloat16,103,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,,,,,,64,8,16,,,True,False,True,True,42,Llama-3.1-70B,True,python3 flashinfer_benchmark.py --routine BatchPrefillWithPagedKVCacheWrapper --backends fa2 fa3 cudnn trtllm-gen --page_size 16 --batch_size 1 --s_qo 1024 --s_kv 1024 --num_qo_heads 64 --num_kv_heads 8 --head_dim_qk 128 --head_dim_vo 128 --random_actual_seq_len -vv --refcheck --causal --q_dtype bfloat16 --kv_dtype bfloat16 --allow_output_mismatch --generate_repro_command --case_tag Llama-3.1-70B
BatchPrefillWithPagedKVCacheWrapper,0.010416,0.00012602601583271074,16.6875821812596,0.36453456221198155,trtllm-gen,1024,1024,128,128,,,True,torch.bfloat16,torch.bfloat16,103,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,,,,,,64,8,16,,,True,False,True,True,42,Llama-3.1-70B,True,python3 flashinfer_benchmark.py --routine BatchPrefillWithPagedKVCacheWrapper --backends fa2 fa3 cudnn trtllm-gen --page_size 16 --batch_size 1 --s_qo 1024 --s_kv 1024 --num_qo_heads 64 --num_kv_heads 8 --head_dim_qk 128 --head_dim_vo 128 --random_actual_seq_len -vv --refcheck --causal --q_dtype bfloat16 --kv_dtype bfloat16 --allow_output_mismatch --generate_repro_command --case_tag Llama-3.1-70B
BatchPrefillWithRaggedKVCacheWrapper,0.5077125,0.0034909562580409865,213.66765797572447,1.6916055444764508,fa2,1024,1024,192,128,,,True,torch.bfloat16,torch.bfloat16,327,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16,,,,,,,128,128,0,,,True,False,True,True,42,DeepSeek-R1,True,python3 flashinfer_benchmark.py --routine BatchPrefillWithRaggedKVCacheWrapper --backends fa2 fa3 cutlass cudnn --batch_size 16 --s_qo 1024 --s_kv 1024 --num_qo_heads 128 --num_kv_heads 128 --head_dim_qk 192 --head_dim_vo 128 --random_actual_seq_len -vv --refcheck --causal --q_dtype bfloat16 --kv_dtype bfloat16 --allow_output_mismatch --generate_repro_command --case_tag DeepSeek-R1
BatchPrefillWithRaggedKVCacheWrapper,0.5157455,0.003936178626395013,210.3396748977936,1.6652579227545372,cutlass,1024,1024,192,128,,,True,torch.bfloat16,torch.bfloat16,327,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16,,,,,,,128,128,0,,,True,False,True,True,42,DeepSeek-R1,True,python3 flashinfer_benchmark.py --routine BatchPrefillWithRaggedKVCacheWrapper --backends fa2 fa3 cutlass cudnn --batch_size 16 --s_qo 1024 --s_kv 1024 --num_qo_heads 128 --num_kv_heads 128 --head_dim_qk 192 --head_dim_vo 128 --random_actual_seq_len -vv --refcheck --causal --q_dtype bfloat16 --kv_dtype bfloat16 --allow_output_mismatch --generate_repro_command --case_tag DeepSeek-R1
BatchPrefillWithRaggedKVCacheWrapper,0.291505,0.0014704125078811516,372.14367094904026,2.9462591722268914,cudnn,1024,1024,192,128,,,True,torch.bfloat16,torch.bfloat16,327,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16,,,,,,,128,128,0,,,True,False,True,True,42,DeepSeek-R1,True,python3 flashinfer_benchmark.py --routine BatchPrefillWithRaggedKVCacheWrapper --backends fa2 fa3 cutlass cudnn --batch_size 16 --s_qo 1024 --s_kv 1024 --num_qo_heads 128 --num_kv_heads 128 --head_dim_qk 192 --head_dim_vo 128 --random_actual_seq_len -vv --refcheck --causal --q_dtype bfloat16 --kv_dtype bfloat16 --allow_output_mismatch --generate_repro_command --case_tag DeepSeek-R1
BatchDecodeWithPagedKVCacheWrapper,0.05048,0.0003465227345435733,5.210553407290016,0.6617052297939778,fa2,1,1024,128,128,,,False,torch.bfloat16,torch.bfloat16,501,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16,,,,,,,64,8,16,,,True,False,True,True,42,Llama-3.1-70B,True,python3 flashinfer_benchmark.py --routine BatchDecodeWithPagedKVCacheWrapper --backends fa2 fa2_tc cudnn trtllm-gen trtllm-gen-native --page_size 16 --batch_size 16 --s_qo 1 --s_kv 1024 --num_qo_heads 64 --num_kv_heads 8 --head_dim_qk 128 --head_dim_vo 128 --random_actual_seq_len -vv --refcheck --q_dtype bfloat16 --kv_dtype bfloat16 --allow_output_mismatch --generate_repro_command --case_tag Llama-3.1-70B
BatchDecodeWithPagedKVCacheWrapper,0.0218245,0.00026476689118291706,12.051993676831083,1.5305221196361884,fa2_tc,1,1024,128,128,,,False,torch.bfloat16,torch.bfloat16,501,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16,,,,,,,64,8,16,,,True,False,True,True,42,Llama-3.1-70B,True,python3 flashinfer_benchmark.py --routine BatchDecodeWithPagedKVCacheWrapper --backends fa2 fa2_tc cudnn trtllm-gen trtllm-gen-native --page_size 16 --batch_size 16 --s_qo 1 --s_kv 1024 --num_qo_heads 64 --num_kv_heads 8 --head_dim_qk 128 --head_dim_vo 128 --random_actual_seq_len -vv --refcheck --q_dtype bfloat16 --kv_dtype bfloat16 --allow_output_mismatch --generate_repro_command --case_tag Llama-3.1-70B
BatchDecodeWithPagedKVCacheWrapper,0.015151999999999999,0.00032976116104437367,17.35934107708553,2.204519535374868,cudnn,1,1024,128,128,,,False,torch.bfloat16,torch.bfloat16,501,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16,,,,,,,64,8,16,,,True,False,True,True,42,Llama-3.1-70B,True,python3 flashinfer_benchmark.py --routine BatchDecodeWithPagedKVCacheWrapper --backends fa2 fa2_tc cudnn trtllm-gen trtllm-gen-native --page_size 16 --batch_size 16 --s_qo 1 --s_kv 1024 --num_qo_heads 64 --num_kv_heads 8 --head_dim_qk 128 --head_dim_vo 128 --random_actual_seq_len -vv --refcheck --q_dtype bfloat16 --kv_dtype bfloat16 --allow_output_mismatch --generate_repro_command --case_tag Llama-3.1-70B
BatchDecodeWithPagedKVCacheWrapper,0.013504,0.00023366085584786245,19.477838862559242,2.473554502369668,trtllm-gen,1,1024,128,128,,,False,torch.bfloat16,torch.bfloat16,501,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16,,,,,,,64,8,16,,,True,False,True,True,42,Llama-3.1-70B,True,python3 flashinfer_benchmark.py --routine BatchDecodeWithPagedKVCacheWrapper --backends fa2 fa2_tc cudnn trtllm-gen trtllm-gen-native --page_size 16 --batch_size 16 --s_qo 1 --s_kv 1024 --num_qo_heads 64 --num_kv_heads 8 --head_dim_qk 128 --head_dim_vo 128 --random_actual_seq_len -vv --refcheck --q_dtype bfloat16 --kv_dtype bfloat16 --allow_output_mismatch --generate_repro_command --case_tag Llama-3.1-70B
BatchDecodeWithPagedKVCacheWrapper,0.013488,0.00026481675844922575,19.50094424673784,2.476488730723606,trtllm-native,1,1024,128,128,,,False,torch.bfloat16,torch.bfloat16,501,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16,,,,,,,64,8,16,,,True,False,True,True,42,Llama-3.1-70B,True,python3 flashinfer_benchmark.py --routine BatchDecodeWithPagedKVCacheWrapper --backends fa2 fa2_tc cudnn trtllm-gen trtllm-gen-native --page_size 16 --batch_size 16 --s_qo 1 --s_kv 1024 --num_qo_heads 64 --num_kv_heads 8 --head_dim_qk 128 --head_dim_vo 128 --random_actual_seq_len -vv --refcheck --q_dtype bfloat16 --kv_dtype bfloat16 --allow_output_mismatch --generate_repro_command --case_tag Llama-3.1-70B
BatchMLAPagedAttentionWrapper,0.024320500000000002,0.0015404488869590362,91.92838287353516,0.9593065932032647,trtllm-native,1,1024,,,512,64,False,torch.bfloat16,torch.bfloat16,501,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16,,,,,,,128,128,32,,,True,False,True,False,42,DeepSeek-R1,True,python3 flashinfer_benchmark.py --routine BatchMLAPagedAttentionWrapper --backends trtllm-gen-native fa2 fa3 --page_size 32 --batch_size 16 --s_qo 1 --s_kv 1024 --num_qo_heads 128 --num_kv_heads 128 --head_dim_ckv 512 --head_dim_kpe 64 --random_actual_seq_len -vv --refcheck --q_dtype bfloat16 --kv_dtype bfloat16 --generate_repro_command --case_tag DeepSeek-R1
BatchMLAPagedAttentionWrapper,0.0391525,0.00025226886891216313,57.103485107421875,0.5958959453419322,fa2,1,1024,,,512,64,False,torch.bfloat16,torch.bfloat16,501,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16,,,,,,,128,128,32,,,True,False,True,False,42,DeepSeek-R1,True,python3 flashinfer_benchmark.py --routine BatchMLAPagedAttentionWrapper --backends trtllm-gen-native fa2 fa3 --page_size 32 --batch_size 16 --s_qo 1 --s_kv 1024 --num_qo_heads 128 --num_kv_heads 128 --head_dim_ckv 512 --head_dim_kpe 64 --random_actual_seq_len -vv --refcheck --q_dtype bfloat16 --kv_dtype bfloat16 --generate_repro_command --case_tag DeepSeek-R1
bmm_fp8,0.08452799999999999,0.00044238307306375754,44.45978118493281,0.08727162597009276,cudnn,,,,,,,,,,,,1024,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,64,,torch.float8_e4m3fn,torch.bfloat16,,4,7168,,,,,,True,False,True,False,42,None,True,python3 flashinfer_benchmark.py --routine bmm_fp8 --batch_size 64 --m 4 --n 1024 --k 7168 --input_dtype fp8_e4m3 --mat2_dtype fp8_e4m3 --out_dtype bfloat16 --backends cudnn cublas cutlass --refcheck -vv --generate_repro_command
bmm_fp8,0.084641,0.0004453848074044147,44.40042513675405,0.0871551139518673,cublas,,,,,,,,,,,,1024,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,64,,torch.float8_e4m3fn,torch.bfloat16,,4,7168,,,,,,True,False,True,False,42,None,True,python3 flashinfer_benchmark.py --routine bmm_fp8 --batch_size 64 --m 4 --n 1024 --k 7168 --input_dtype fp8_e4m3 --mat2_dtype fp8_e4m3 --out_dtype bfloat16 --backends cudnn cublas cutlass --refcheck -vv --generate_repro_command
bmm_fp8,0.0862085,0.00042494313998724884,43.59310722260566,0.08557040199052297,cutlass,,,,,,,,,,,,1024,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,64,,torch.float8_e4m3fn,torch.bfloat16,,4,7168,,,,,,True,False,True,False,42,None,True,python3 flashinfer_benchmark.py --routine bmm_fp8 --batch_size 64 --m 4 --n 1024 --k 7168 --input_dtype fp8_e4m3 --mat2_dtype fp8_e4m3 --out_dtype bfloat16 --backends cudnn cublas cutlass --refcheck -vv --generate_repro_command
gemm_fp8_nt_groupwise,0.016048,0.00011820765156659246,14.636155533399801,0.46656829511465603,cutlass,,,,,,,,,,,,1024,,128,MN,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,fp8_e4m3,torch.bfloat16,,16,7168,,,,,,True,False,True,False,42,None,True,python3 flashinfer_benchmark.py --routine gemm_fp8_nt_groupwise --m 16 --n 1024 --k 7168 --mma_sm 1 --scale_major_mode MN --backends cutlass --refcheck -vv --generate_repro_command
group_gemm_fp8_nt_groupwise,0.0228325,0.0001976228984707995,20.574271236176504,0.6558623015438519,cutlass,,,,,,,,,,,,1024,2,128,MN,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,fp8_e4m3,torch.bfloat16,,16,7168,,,,,,True,False,True,False,42,None,True,python3 flashinfer_benchmark.py --routine group_gemm_fp8_nt_groupwise --m 16 --n 1024 --k 7168 --mma_sm 1 --group_size 2 --scale_major_mode MN --refcheck -vv --generate_repro_command
mm_fp4,0.009152,0.00011045976643104036,821.2623216783217,0.7160839160839161,cudnn,,,,,,,,,,,,1024,,,,,True,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,fp8_e4m3,torch.bfloat16,,512,7168,,,,,,True,False,True,False,42,None,True,python3 flashinfer_benchmark.py --routine mm_fp4 --m 512 --n 1024 --k 7168 --out_dtype bfloat16 --backends cudnn cutlass trtllm --use_128x4_sf_layout --use_nvfp4 --refcheck -vv --generate_repro_command
mm_fp4,0.010176,9.755467982396113e-05,738.619572327044,0.6440251572327044,cutlass,,,,,,,,,,,,1024,,,,,True,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,fp8_e4m3,torch.bfloat16,,512,7168,,,,,,True,False,True,False,42,None,True,python3 flashinfer_benchmark.py --routine mm_fp4 --m 512 --n 1024 --k 7168 --out_dtype bfloat16 --backends cudnn cutlass trtllm --use_128x4_sf_layout --use_nvfp4 --refcheck -vv --generate_repro_command
mm_fp4,0.01184,0.00017707846346244992,634.8135783783783,0.5535135135135135,trtllm,,,,,,,,,,,,1024,,,,,True,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,fp8_e4m3,torch.bfloat16,,512,7168,,,,,,True,False,True,False,42,None,True,python3 flashinfer_benchmark.py --routine mm_fp4 --m 512 --n 1024 --k 7168 --out_dtype bfloat16 --backends cudnn cutlass trtllm --use_128x4_sf_layout --use_nvfp4 --refcheck -vv --generate_repro_command
mm_fp4,0.0091355,0.00014340164651154523,822.745637129878,0.7173772645175415,cudnn_autotune,,,,,,,,,,,,1024,,,,,True,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,fp8_e4m3,torch.bfloat16,,512,7168,,,,,,True,False,True,False,42,None,True,python3 flashinfer_benchmark.py --routine mm_fp4 --m 512 --n 1024 --k 7168 --out_dtype bfloat16 --backends cudnn cutlass trtllm --use_128x4_sf_layout --use_nvfp4 --autotune --refcheck -vv --generate_repro_command
mm_fp4,0.009936,0.00014641446498060083,756.4606247987117,0.6595813204508857,cutlass_autotune,,,,,,,,,,,,1024,,,,,True,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,fp8_e4m3,torch.bfloat16,,512,7168,,,,,,True,False,True,False,42,None,True,python3 flashinfer_benchmark.py --routine mm_fp4 --m 512 --n 1024 --k 7168 --out_dtype bfloat16 --backends cudnn cutlass trtllm --use_128x4_sf_layout --use_nvfp4 --autotune --refcheck -vv --generate_repro_command
mm_fp4,0.014048,0.00016289848577155853,535.0365011389522,0.46651480637813214,trtllm_autotune,,,,,,,,,,,,1024,,,,,True,True,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,fp8_e4m3,torch.bfloat16,,512,7168,,,,,,True,False,True,False,42,None,True,python3 flashinfer_benchmark.py --routine mm_fp4 --m 512 --n 1024 --k 7168 --out_dtype bfloat16 --backends cudnn cutlass trtllm --use_128x4_sf_layout --use_nvfp4 --autotune --refcheck -vv --generate_repro_command
trtllm_fp4_block_scale_moe,0.131376,0.0007720556427846086,392.3061103397881,3.476436974789916,trtllm,,,,,,,,,,,,,,,,,,,1024,1024,256,8,8,4,2.5,0,256,deepseek_v3,True,0,True,False,torch.bfloat16,swiglu,,,,,,,1024,256,8,,,,,,,,,,,,,8,,,,,,,,,,,,,,,,,1024,torch.bfloat16,,,,,,,,,,False,False,True,False,42,trtllm_moe_sample,True,python3 flashinfer_benchmark.py --routine trtllm_fp4_block_scale_moe --num_tokens 1024 --hidden_size 1024 --intermediate_size 1024 --num_experts 256 --top_k 8 --n_group 8 --topk_group 4 --routed_scaling_factor 2.5 --use_routing_bias --routing_method deepseek_v3 --use_shuffled_weight -vv --generate_repro_command --case_tag trtllm_moe_sample
trtllm_fp4_block_scale_moe,0.09689600000000001,0.000823255195347509,531.9064517833552,2.367915455746367,trtllm,,,,,,,,,,,,,,,,,,,1024,1024,128,8,None,None,2.5,0,128,renormalize_naive,True,0,False,False,torch.bfloat16,swiglu,,,,,,,1024,128,8,,,,,,,,,,,,,8,,,,,,,,,,,,,,,,,1024,torch.bfloat16,,,,,,,,,,False,False,True,False,42,trtllm_moe_sample,True,python3 flashinfer_benchmark.py --routine trtllm_fp4_block_scale_moe --num_tokens 1024 --hidden_size 1024 --intermediate_size 1024 --num_experts 128 --top_k 8 --routing_method renormalize_naive --use_shuffled_weight -vv --generate_repro_command --case_tag trtllm_moe_sample
trtllm_fp8_block_scale_moe,0.30302450000000003,0.001741246330138908,170.08396202947284,2.67140337497463,trtllm,,,,,,,,,,,,,,,,,,,1024,1024,256,8,8,4,2.5,0,256,deepseek_v3,True,0,True,False,torch.bfloat16,,,,,,,,1024,256,8,,,,,,,,,,,,,8,,,,,,,,,,,,,,,,,1024,torch.bfloat16,,,,,,,,,,False,False,True,False,42,trtllm_moe_sample,True,python3 flashinfer_benchmark.py --routine trtllm_fp8_block_scale_moe --num_tokens 1024 --hidden_size 1024 --intermediate_size 1024 --num_experts 256 --top_k 8 --n_group 8 --topk_group 4 --routed_scaling_factor 2.5 --use_routing_bias --routing_method deepseek_v3 --use_shuffled_weight -vv --generate_repro_command --case_tag trtllm_moe_sample
trtllm_fp8_per_tensor_scale_moe,0.092,0.00038124563239756883,70.02664069565218,4.413707130434783,trtllm,,,,,,,,,,,,,,,,,,,1024,1024,128,1,None,None,2.5,0,128,llama4,,,True,True,torch.bfloat16,,,,,,,,1024,128,1,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,1024,torch.bfloat16,,,,,,,,,,False,False,True,False,42,trtllm_moe_sample,True,python3 flashinfer_benchmark.py --routine trtllm_fp8_per_tensor_scale_moe --num_tokens 1024 --hidden_size 1024 --intermediate_size 1024 --num_experts 128 --top_k 1 --routed_scaling_factor 2.5 --use_routing_bias --routing_method llama4 --use_routing_scales_on_input -vv --generate_repro_command --case_tag trtllm_moe_sample
cutlass_fused_moe,0.027808,0.0002716811468533574,0.22624626006904489,0.007663981588032221,cutlass,,,,,,,,,,,,,,,,,,,32,128,2,2,,,,,,,False,0,,False,torch.float16,,base,False,1,0,1,0,32,2,2,1,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,128,torch.float16,,,,,,,,,,False,False,True,False,42,cutlass_moe_base,True,python3 flashinfer_benchmark.py --routine cutlass_fused_moe --num_tokens 32 --hidden_size 128 --intermediate_size 128 --num_experts 2 --top_k 2 --cutlass_variant base --input_dtype float16 -vv --generate_repro_command --case_tag cutlass_moe_base
cutlass_fused_moe,0.028432,0.0001927351348929982,0.22128081035453012,0.00389420371412493,cutlass,,,,,,,,,,,,,,,,,,,32,128,2,2,,,,,,,False,0,,False,torch.float16,,fp8,False,1,0,1,0,32,2,2,1,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,128,torch.float16,,,,,,,,,,False,False,True,False,42,cutlass_moe_fp8_scale,True,python3 flashinfer_benchmark.py --routine cutlass_fused_moe --num_tokens 32 --hidden_size 128 --intermediate_size 128 --num_experts 2 --top_k 2 --cutlass_variant fp8 --input_dtype float16 -vv --generate_repro_command --case_tag cutlass_moe_fp8_scale
cutlass_fused_moe,0.029984,0.0003416674146333277,0.20982710779082178,0.002198505869797225,cutlass,,,,,,,,,,,,,,,,,,,32,128,2,2,,,,,,,False,0,,False,torch.float16,,nvfp4,False,1,0,1,0,32,2,2,1,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,128,torch.float16,,,,,,,,,,False,False,True,False,42,cutlass_moe_nvfp4_weights,True,python3 flashinfer_benchmark.py --routine cutlass_fused_moe --num_tokens 32 --hidden_size 128 --intermediate_size 128 --num_experts 2 --top_k 2 --cutlass_variant nvfp4 --input_dtype float16 -vv --generate_repro_command --case_tag cutlass_moe_nvfp4_weights
cutlass_fused_moe,0.028319999999999998,0.0003103376584016549,0.22215593220338983,0.002327683615819209,cutlass,,,,,,,,,,,,,,,,,,,32,128,2,2,,,,,,,False,0,,False,torch.float16,,nvfp4,True,1,0,1,0,32,2,2,1,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,128,torch.float16,,,,,,,,,,False,False,True,False,42,cutlass_moe_nvfp4_weights_quantized,True,python3 flashinfer_benchmark.py --routine cutlass_fused_moe --num_tokens 32 --hidden_size 128 --intermediate_size 128 --num_experts 2 --top_k 2 --cutlass_variant nvfp4 --quantized_input --input_dtype float16 -vv --generate_repro_command --case_tag cutlass_moe_nvfp4_weights_quantized
cutlass_fused_moe,0.026368,0.0002925743988952024,0.23860194174757282,0.03046601941747573,cutlass,,,,,,,,,,,,,,,,,,,32,128,8,2,,,,,,,False,0,,False,torch.float16,,base,False,2,0,4,0,32,8,2,4,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,128,torch.float16,,,,,,,,,,False,False,True,False,42,cutlass_moe_nvfp4_ep_tp,True,python3 flashinfer_benchmark.py --routine cutlass_fused_moe --num_tokens 32 --hidden_size 128 --intermediate_size 128 --num_experts 8 --top_k 2 --cutlass_variant base --input_dtype float16 --tp_size 2 --tp_rank 0 --ep_size 4 --ep_rank 0 -vv --generate_repro_command --case_tag cutlass_moe_nvfp4_ep_tp
rmsnorm,0.002912,7.812019940806316e-05,0.22505494505494505,0.18285714285714286,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,fp8_e4m3,,,,,,,False,False,True,False,True,False,42,rmsnorm_llama_hidden,True,python3 flashinfer_benchmark.py --routine rmsnorm --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 --refcheck -vv --generate_repro_command --case_tag rmsnorm_llama_hidden
rmsnorm,0.003424,8.368695637114948e-05,0.765607476635514,0.6172710280373832,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,,,,,,,,,,,,,,,,,,,,,,,,64,8192,torch.bfloat16,fp8_e4m3,,,,,,,False,False,True,False,True,False,42,rmsnorm_large_hidden,True,python3 flashinfer_benchmark.py --routine rmsnorm --batch_size 64 --hidden_size 8192 --input_dtype bfloat16 --refcheck -vv --generate_repro_command --case_tag rmsnorm_large_hidden
rmsnorm,0.00256,7.790495633926137e-05,0.256,0.2049,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,,1e-06,,,,,,,,,,,,,,,,,,,,,,,,32,128,torch.bfloat16,fp8_e4m3,,,,,,,False,False,True,False,True,False,42,rmsnorm_3d_gqa,True,python3 flashinfer_benchmark.py --routine rmsnorm --batch_size 32 --num_heads 32 --hidden_size 128 --input_dtype bfloat16 --refcheck -vv --generate_repro_command --case_tag rmsnorm_3d_gqa
rmsnorm,0.00256,8.524028781431153e-05,0.256,0.2049,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,64,,1e-06,,,,,,,,,,,,,,,,,,,,,,,,16,128,torch.float16,fp8_e4m3,,,,,,,False,False,True,False,True,False,42,rmsnorm_3d_mha,True,python3 flashinfer_benchmark.py --routine rmsnorm --batch_size 16 --num_heads 64 --hidden_size 128 --input_dtype float16 --refcheck -vv --generate_repro_command --case_tag rmsnorm_3d_mha
rmsnorm,0.002912,0.00011321305382134853,0.22505494505494505,0.18285714285714286,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,fp8_e4m3,,,,,,,True,False,True,False,True,False,42,rmsnorm_pdl,True,python3 flashinfer_benchmark.py --routine rmsnorm --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 --enable_pdl --refcheck -vv --generate_repro_command --case_tag rmsnorm_pdl
rmsnorm_quant,0.002944,5.0394929198173184e-05,0.22260869565217392,0.13634782608695653,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,1e-06,,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,torch.float8_e4m3fn,,,,,,,False,False,True,False,True,False,42,rmsnorm_quant_fp8_e4m3,True,python3 flashinfer_benchmark.py --routine rmsnorm_quant --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 --out_dtype fp8_e4m3 --scale 1.0 --refcheck -vv --generate_repro_command --case_tag rmsnorm_quant_fp8_e4m3
rmsnorm_quant,0.0034714999999999998,8.928969705402746e-05,0.7551317874117818,0.45779864611839266,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,1e-06,,,,,,,,,,,,,,,,,,,,,,,,64,8192,torch.bfloat16,torch.float8_e4m3fn,,,,,,,False,False,True,False,True,False,42,rmsnorm_quant_large,True,python3 flashinfer_benchmark.py --routine rmsnorm_quant --batch_size 64 --hidden_size 8192 --input_dtype bfloat16 --out_dtype fp8_e4m3 --scale 1.0 --refcheck -vv --generate_repro_command --case_tag rmsnorm_quant_large
rmsnorm_quant,0.002912,0.00010142936239351772,0.22505494505494505,0.13784615384615384,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,1e-06,,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.float16,torch.float8_e5m2,,,,,,,False,False,True,False,True,False,42,rmsnorm_quant_fp8_e5m2,True,python3 flashinfer_benchmark.py --routine rmsnorm_quant --batch_size 32 --hidden_size 4096 --input_dtype float16 --out_dtype fp8_e5m2 --scale 1.0 --refcheck -vv --generate_repro_command --case_tag rmsnorm_quant_fp8_e5m2
fused_add_rmsnorm_quant,0.003232,0.00010254337293717882,0.24332673267326732,0.2864158415841584,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,1e-06,,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,torch.float8_e4m3fn,,,,,,,False,False,True,False,True,False,42,fused_add_rmsnorm_quant_fp8_e4m3,True,python3 flashinfer_benchmark.py --routine fused_add_rmsnorm_quant --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 --out_dtype fp8_e4m3 --scale 1.0 --refcheck -vv --generate_repro_command --case_tag fused_add_rmsnorm_quant_fp8_e4m3
fused_add_rmsnorm_quant,0.003936,6.175634020532265e-05,0.7992195121951219,0.9365853658536585,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,1e-06,,,,,,,,,,,,,,,,,,,,,,,,64,8192,torch.bfloat16,torch.float8_e4m3fn,,,,,,,False,False,True,False,True,False,42,fused_add_rmsnorm_quant_large,True,python3 flashinfer_benchmark.py --routine fused_add_rmsnorm_quant --batch_size 64 --hidden_size 8192 --input_dtype bfloat16 --out_dtype fp8_e4m3 --scale 1.0 --refcheck -vv --generate_repro_command --case_tag fused_add_rmsnorm_quant_large
fused_add_rmsnorm_quant,0.0032,8.371249740762858e-05,0.24576,0.28928,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,1e-06,,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,torch.float8_e4m3fn,,,,,,,True,False,True,False,True,False,42,fused_add_rmsnorm_quant_pdl,True,python3 flashinfer_benchmark.py --routine fused_add_rmsnorm_quant --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 --out_dtype fp8_e4m3 --scale 1.0 --enable_pdl --refcheck -vv --generate_repro_command --case_tag fused_add_rmsnorm_quant_pdl
rmsnorm_fp4quant,0.006016499999999999,0.00015377948136500162,0.10892711709465638,0.0571867364746946,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,False,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,nvfp4,,,,,,,False,False,False,False,True,False,42,rmsnorm_fp4quant_nvfp4,True,python3 flashinfer_benchmark.py --routine rmsnorm_fp4quant --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 -vv --generate_repro_command --case_tag rmsnorm_fp4quant_nvfp4
rmsnorm_fp4quant,0.006656,0.00010579418698586425,0.39384615384615385,0.2043076923076923,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,False,,,,,,,,,,,,,,,,,,,,,,,64,8192,torch.bfloat16,nvfp4,,,,,,,False,False,False,False,True,False,42,rmsnorm_fp4quant_nvfp4_large,True,python3 flashinfer_benchmark.py --routine rmsnorm_fp4quant --batch_size 64 --hidden_size 8192 --input_dtype bfloat16 -vv --generate_repro_command --case_tag rmsnorm_fp4quant_nvfp4_large
rmsnorm_fp4quant,0.004512,0.00012588926350831783,0.1452482269503546,0.07625531914893617,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,True,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,nvfp4,,,,,,,False,False,False,False,True,False,42,rmsnorm_fp4quant_nvfp4_global,True,python3 flashinfer_benchmark.py --routine rmsnorm_fp4quant --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 --use_global_scale -vv --generate_repro_command --case_tag rmsnorm_fp4quant_nvfp4_global
rmsnorm_fp4quant,0.0063035,0.00011677332743396502,0.10396763702704846,0.05458300943920044,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,False,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,nvfp4,,,,,,,False,True,False,False,True,False,42,rmsnorm_fp4quant_nvfp4_swizzled,True,python3 flashinfer_benchmark.py --routine rmsnorm_fp4quant --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 --is_sf_swizzled_layout -vv --generate_repro_command --case_tag rmsnorm_fp4quant_nvfp4_swizzled
rmsnorm_fp4quant,0.005344,0.00021772911049181175,0.12263473053892215,0.06361676646706586,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,False,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,mxfp4,,,,,,,False,False,False,False,True,False,42,rmsnorm_fp4quant_mxfp4,True,python3 flashinfer_benchmark.py --routine rmsnorm_fp4quant --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 --out_dtype mxfp4 -vv --generate_repro_command --case_tag rmsnorm_fp4quant_mxfp4
rmsnorm_fp4quant,0.004032,0.00012955181288666794,0.16253968253968254,0.08336507936507936,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,,1e-06,False,,,,,,,,,,,,,,,,,,,,,,,32,128,torch.bfloat16,nvfp4,,,,,,,False,False,False,False,True,False,42,rmsnorm_fp4quant_3d,True,python3 flashinfer_benchmark.py --routine rmsnorm_fp4quant --batch_size 32 --num_heads 32 --hidden_size 128 --input_dtype bfloat16 -vv --generate_repro_command --case_tag rmsnorm_fp4quant_3d
add_rmsnorm_fp4quant,0.005248,0.00010570372115808723,0.14985365853658536,0.16546341463414635,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,False,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,nvfp4,,,,,,,False,False,False,False,True,False,42,add_rmsnorm_fp4quant_nvfp4,True,python3 flashinfer_benchmark.py --routine add_rmsnorm_fp4quant --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 -vv --generate_repro_command --case_tag add_rmsnorm_fp4quant_nvfp4
add_rmsnorm_fp4quant,0.005792,0.00010571013301581935,0.5431160220994475,0.5968618784530386,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,False,,,,,,,,,,,,,,,,,,,,,,,64,8192,torch.bfloat16,nvfp4,,,,,,,False,False,False,False,True,False,42,add_rmsnorm_fp4quant_nvfp4_large,True,python3 flashinfer_benchmark.py --routine add_rmsnorm_fp4quant --batch_size 64 --hidden_size 8192 --input_dtype bfloat16 -vv --generate_repro_command --case_tag add_rmsnorm_fp4quant_nvfp4_large
add_rmsnorm_fp4quant,0.003616,8.856776438913255e-05,0.21748672566371682,0.24014159292035397,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,True,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,nvfp4,,,,,,,False,False,False,False,True,False,42,add_rmsnorm_fp4quant_nvfp4_global,True,python3 flashinfer_benchmark.py --routine add_rmsnorm_fp4quant --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 --use_global_scale -vv --generate_repro_command --case_tag add_rmsnorm_fp4quant_nvfp4_global
add_rmsnorm_fp4quant,0.005536,0.00010942230120044093,0.1420578034682081,0.15685549132947976,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,False,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,nvfp4,,,,,,,False,True,False,False,True,False,42,add_rmsnorm_fp4quant_nvfp4_swizzled,True,python3 flashinfer_benchmark.py --routine add_rmsnorm_fp4quant --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 --is_sf_swizzled_layout -vv --generate_repro_command --case_tag add_rmsnorm_fp4quant_nvfp4_swizzled
add_rmsnorm_fp4quant,0.005344,0.00015592187858739457,0.1471616766467066,0.1617245508982036,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,False,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,mxfp4,,,,,,,False,False,False,False,True,False,42,add_rmsnorm_fp4quant_mxfp4,True,python3 flashinfer_benchmark.py --routine add_rmsnorm_fp4quant --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 --out_dtype mxfp4 -vv --generate_repro_command --case_tag add_rmsnorm_fp4quant_mxfp4
add_rmsnorm_fp4quant,0.003968,0.0001344020667830504,0.19819354838709677,0.21683870967741936,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,,1e-06,False,,,,,,,,,,,,,,,,,,,,,,,32,128,torch.bfloat16,nvfp4,,,,,,,False,False,False,False,True,False,42,add_rmsnorm_fp4quant_3d,True,python3 flashinfer_benchmark.py --routine add_rmsnorm_fp4quant --batch_size 32 --num_heads 32 --hidden_size 128 --input_dtype bfloat16 -vv --generate_repro_command --case_tag add_rmsnorm_fp4quant_3d
add_rmsnorm_fp4quant,0.005248,0.00013531114760679053,0.14985365853658536,0.16702439024390245,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,False,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,nvfp4,,,,,,,False,False,False,False,True,False,42,add_rmsnorm_fp4quant_both_sf,True,python3 flashinfer_benchmark.py --routine add_rmsnorm_fp4quant --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 --output_both_sf_layouts -vv --generate_repro_command --case_tag add_rmsnorm_fp4quant_both_sf
add_rmsnorm_fp4quant,0.005632,0.00013822792771361366,0.5585454545454546,0.6196363636363637,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,False,,,,,,,,,,,,,,,,,,,,,,,64,8192,torch.bfloat16,nvfp4,,,,,,,False,False,False,False,True,False,42,add_rmsnorm_fp4quant_both_sf_large,True,python3 flashinfer_benchmark.py --routine add_rmsnorm_fp4quant --batch_size 64 --hidden_size 8192 --input_dtype bfloat16 --output_both_sf_layouts -vv --generate_repro_command --case_tag add_rmsnorm_fp4quant_both_sf_large
add_rmsnorm_fp4quant,0.0037914999999999997,9.353935000843226e-05,0.20741975471449298,0.23118660160886195,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,True,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,nvfp4,,,,,,,False,False,False,False,True,False,42,add_rmsnorm_fp4quant_both_sf_global,True,python3 flashinfer_benchmark.py --routine add_rmsnorm_fp4quant --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 --use_global_scale --output_both_sf_layouts -vv --generate_repro_command --case_tag add_rmsnorm_fp4quant_both_sf_global
add_rmsnorm_fp4quant,0.005376,9.779302974479657e-05,0.1462857142857143,0.16152380952380951,cute-dsl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1e-06,False,,,,,,,,,,,,,,,,,,,,,,,32,4096,torch.bfloat16,mxfp4,,,,,,,False,False,False,False,True,False,42,add_rmsnorm_fp4quant_mxfp4_both_sf,True,python3 flashinfer_benchmark.py --routine add_rmsnorm_fp4quant --batch_size 32 --hidden_size 4096 --input_dtype bfloat16 --out_dtype mxfp4 --output_both_sf_layouts -vv --generate_repro_command --case_tag add_rmsnorm_fp4quant_mxfp4_both_sf
mxfp8_quantize,0.006784,7.182856906087795e-05,1.8547924528301887,1.8741132075471698,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,,,,,,,,,,,,,,,,,,,,,,None,,torch.bfloat16,,,1024,4096,,,,False,True,False,False,True,False,42,mxfp8_quantize_basic,True,python3 flashinfer_benchmark.py --routine mxfp8_quantize --m 1024 --k 4096 --input_dtype bfloat16 -vv --generate_repro_command --case_tag mxfp8_quantize_basic
mxfp8_quantize,0.014832,0.00021055667223391944,3.3934498381877023,3.4287982740021574,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,,,,,,,,,,,,,,,,,,,,,,None,,torch.bfloat16,,,2048,8192,,,,False,True,False,False,True,False,42,mxfp8_quantize_large,True,python3 flashinfer_benchmark.py --routine mxfp8_quantize --m 2048 --k 8192 --input_dtype bfloat16 -vv --generate_repro_command --case_tag mxfp8_quantize_large
mxfp8_quantize,0.00672,0.00012175248115199402,1.8724571428571428,1.8919619047619047,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,,,,,,,,,,,,,,,,,,,,,,None,,torch.float16,,,1024,4096,,,,False,True,False,False,True,False,42,mxfp8_quantize_fp16,True,python3 flashinfer_benchmark.py --routine mxfp8_quantize --m 1024 --k 4096 --input_dtype float16 -vv --generate_repro_command --case_tag mxfp8_quantize_fp16
mxfp8_quantize,0.00656,0.00015712367386517179,1.9181268292682927,1.9381073170731706,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,,,,,,,,,,,,,,,,,,,,,,None,,torch.bfloat16,,,1024,4096,,,,False,False,False,False,True,False,42,mxfp8_quantize_no_swizzle,True,python3 flashinfer_benchmark.py --routine mxfp8_quantize --m 1024 --k 4096 --input_dtype bfloat16 --no_sf_swizzled_layout -vv --generate_repro_command --case_tag mxfp8_quantize_no_swizzle
mxfp8_quantize,0.014864,0.00017424996413199052,3.3861442411194833,3.4214165769644778,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,,,,,,,,,,,,,,,,,,,,,,None,,torch.bfloat16,,,2048,8192,,,,True,True,False,False,True,False,42,mxfp8_quantize_pdl,True,python3 flashinfer_benchmark.py --routine mxfp8_quantize --m 2048 --k 8192 --input_dtype bfloat16 --enable_pdl -vv --generate_repro_command --case_tag mxfp8_quantize_pdl
mxfp8_quantize,0.006816,0.0001178738685582555,1.8460845070422536,1.8653145539906104,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,,,,,,,,,,,,,,,,,,,,,,None,,torch.bfloat16,,,1024,4096,,,,False,True,True,False,True,False,42,mxfp8_quantize_refcheck,True,python3 flashinfer_benchmark.py --routine mxfp8_quantize --m 1024 --k 4096 --input_dtype bfloat16 --refcheck -vv --generate_repro_command --case_tag mxfp8_quantize_refcheck
mxfp4_quantize,0.048032,0.0005276333280687346,0.26196935376415725,0.22103664223850766,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,None,,torch.bfloat16,,,1024,4096,,,,False,True,False,False,True,False,42,mxfp4_quantize_basic,True,python3 flashinfer_benchmark.py --routine mxfp4_quantize --m 1024 --k 4096 --input_dtype bfloat16 -vv --generate_repro_command --case_tag mxfp4_quantize_basic
mxfp4_quantize,0.12345600000000001,0.0011042029307252459,0.4076889580093312,0.3439875583203732,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,None,,torch.bfloat16,,,2048,8192,,,,False,True,False,False,True,False,42,mxfp4_quantize_large,True,python3 flashinfer_benchmark.py --routine mxfp4_quantize --m 2048 --k 8192 --input_dtype bfloat16 -vv --generate_repro_command --case_tag mxfp4_quantize_large
mxfp4_quantize,0.048656000000000005,0.00037438014548495135,0.25860966787241035,0.21820190726734623,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,None,,torch.bfloat16,,,1024,4096,,,,False,True,True,False,True,False,42,mxfp4_quantize_refcheck,True,python3 flashinfer_benchmark.py --routine mxfp4_quantize --m 1024 --k 4096 --input_dtype bfloat16 --refcheck -vv --generate_repro_command --case_tag mxfp4_quantize_refcheck
nvfp4_quantize,0.0079835,0.0001245252138680712,1.576114736644329,1.346265171917079,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,128x4,False,16,,,,,,,,,,,,,,,,,,None,,torch.bfloat16,,,1024,4096,,,,False,True,False,False,True,False,42,nvfp4_quantize_128x4,True,python3 flashinfer_benchmark.py --routine nvfp4_quantize --m 1024 --k 4096 --input_dtype bfloat16 --global_scale 1.0 --sf_layout 128x4 -vv --generate_repro_command --case_tag nvfp4_quantize_128x4
nvfp4_quantize,0.0138085,0.000238419427340419,3.6449757757902743,3.1134170981641742,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,128x4,False,16,,,,,,,,,,,,,,,,,,None,,torch.bfloat16,,,2048,8192,,,,False,True,False,False,True,False,42,nvfp4_quantize_128x4_large,True,python3 flashinfer_benchmark.py --routine nvfp4_quantize --m 2048 --k 8192 --input_dtype bfloat16 --global_scale 1.0 --sf_layout 128x4 -vv --generate_repro_command --case_tag nvfp4_quantize_128x4_large
nvfp4_quantize,0.008,0.0001464365467437081,1.572864,1.3434885,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,8x4,False,16,,,,,,,,,,,,,,,,,,None,,torch.bfloat16,,,1024,4096,,,,False,True,False,False,True,False,42,nvfp4_quantize_8x4,True,python3 flashinfer_benchmark.py --routine nvfp4_quantize --m 1024 --k 4096 --input_dtype bfloat16 --global_scale 1.0 --sf_layout 8x4 -vv --generate_repro_command --case_tag nvfp4_quantize_8x4
nvfp4_quantize,3.7088235000000003,0.0300100512421533,0.003392696363145887,0.002897929222029573,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,128x4,True,16,,,,,,,,,,,,,,,,,,None,,torch.bfloat16,,,1024,4096,,,,False,True,False,False,True,False,42,nvfp4_quantize_shuffle,True,python3 flashinfer_benchmark.py --routine nvfp4_quantize --m 1024 --k 4096 --input_dtype bfloat16 --global_scale 1.0 --do_shuffle -vv --generate_repro_command --case_tag nvfp4_quantize_shuffle
nvfp4_quantize,0.008031,0.0001525645073039234,1.566792678371311,1.3383025775121404,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,128x4,False,16,,,,,,,,,,,,,,,,,,None,,torch.bfloat16,,,1024,4096,,,,True,True,False,False,True,False,42,nvfp4_quantize_pdl,True,python3 flashinfer_benchmark.py --routine nvfp4_quantize --m 1024 --k 4096 --input_dtype bfloat16 --global_scale 1.0 --enable_pdl -vv --generate_repro_command --case_tag nvfp4_quantize_pdl
nvfp4_batched_quantize,0.021888,0.0001485392727717338,2.2995087719298244,1.9641639254385965,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,16,,,,,,,,,,,,,,,,,,4,,torch.bfloat16,,,1024,4096,,,,False,True,False,False,True,False,42,nvfp4_batched_basic,True,python3 flashinfer_benchmark.py --routine nvfp4_batched_quantize --batch_size 4 --m 1024 --k 4096 --input_dtype bfloat16 --global_scale 1.0 -vv --generate_repro_command --case_tag nvfp4_batched_basic
nvfp4_batched_quantize,0.07880000000000001,0.0006070973562782165,5.10981197969543,4.364631116751268,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,16,,,,,,,,,,,,,,,,,,8,,torch.bfloat16,,,2048,8192,,,,False,True,False,False,True,False,42,nvfp4_batched_large,True,python3 flashinfer_benchmark.py --routine nvfp4_batched_quantize --batch_size 8 --m 2048 --k 8192 --input_dtype bfloat16 --global_scale 1.0 -vv --generate_repro_command --case_tag nvfp4_batched_large
nvfp4_batched_quantize,0.01984,0.00012693900985206308,2.536877419354839,2.166916330645161,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,16,,,,,,,,,,,,,,,,,,4,,torch.float16,,,1024,4096,,,,False,True,False,False,True,False,42,nvfp4_batched_fp16,True,python3 flashinfer_benchmark.py --routine nvfp4_batched_quantize --batch_size 4 --m 1024 --k 4096 --input_dtype float16 --global_scale 1.0 -vv --generate_repro_command --case_tag nvfp4_batched_fp16
softmax,0.012064,0.00015532180214709794,0,0.6790450928381963,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32000,,,,1.0,,,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,softmax_llama,True,python3 flashinfer_benchmark.py --routine softmax --batch_size 32 --vocab_size 32000 --temperature 1.0 --input_dtype float32 -vv --generate_repro_command --case_tag softmax_llama
softmax,0.035552,0.0003970301192045208,0,1.847071107110711,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128256,,,,0.8,,,,,,,,,,,,,64,,float32,,,,,,,,,,False,False,True,False,42,softmax_llama3_temp,True,python3 flashinfer_benchmark.py --routine softmax --batch_size 64 --vocab_size 128256 --temperature 0.8 --input_dtype float32 -vv --generate_repro_command --case_tag softmax_llama3_temp
sampling_from_probs,0.0135365,0.0001606484222006415,0,0.3025987515236583,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32000,,,,,,,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,sampling_from_probs_llama,True,python3 flashinfer_benchmark.py --routine sampling_from_probs --batch_size 32 --vocab_size 32000 -vv --generate_repro_command --case_tag sampling_from_probs_llama
sampling_from_probs,0.042753,0.00035013663364781225,0,0.7679880242322176,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128256,,,,,,,,,,,,,,,,,64,,float32,,,,,,,,,,False,False,True,False,42,sampling_from_probs_llama3,True,python3 flashinfer_benchmark.py --routine sampling_from_probs --batch_size 64 --vocab_size 128256 -vv --generate_repro_command --case_tag sampling_from_probs_llama3
sampling_from_logits,0.0161925,0.00013220608491627353,0,0.2529645206113942,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32000,,,,,,,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,sampling_from_logits_llama,True,python3 flashinfer_benchmark.py --routine sampling_from_logits --batch_size 32 --vocab_size 32000 --input_dtype float32 -vv --generate_repro_command --case_tag sampling_from_logits_llama
sampling_from_logits,0.07783999999999999,0.00028485495139339475,0,0.21090729701952726,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128256,,,,,,,,,,,,,,,,,64,,bfloat16,,,,,,,,,,False,False,True,False,42,sampling_from_logits_llama3,True,python3 flashinfer_benchmark.py --routine sampling_from_logits --batch_size 64 --vocab_size 128256 --input_dtype bfloat16 -vv --generate_repro_command --case_tag sampling_from_logits_llama3
top_k_sampling_from_probs,0.1495995,0.0001935201425060321,0,0.02738062627214663,cuda,,,,,,,,,,,,,,,,,,,,,,50,,,,,,,,,,,,,,,,,,,,,50,,,,,,,,,,,,32000,50,,,,,,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,top_k_sampling_k50,True,python3 flashinfer_benchmark.py --routine top_k_sampling_from_probs --batch_size 32 --vocab_size 32000 --top_k 50 -vv --generate_repro_command --case_tag top_k_sampling_k50
top_k_sampling_from_probs,0.498001,0.0017754358053415802,0,0.03296558842251321,cuda,,,,,,,,,,,,,,,,,,,,,,100,,,,,,,,,,,,,,,,,,,,,100,,,,,,,,,,,,128256,100,,,,,,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,top_k_sampling_k100,True,python3 flashinfer_benchmark.py --routine top_k_sampling_from_probs --batch_size 32 --vocab_size 128256 --top_k 100 -vv --generate_repro_command --case_tag top_k_sampling_k100
top_p_sampling_from_probs,0.022912,0.00022344903719242592,0,0.17877653631284915,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32000,,0.9,,,,,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,top_p_sampling_p09,True,python3 flashinfer_benchmark.py --routine top_p_sampling_from_probs --batch_size 32 --vocab_size 32000 --top_p 0.9 -vv --generate_repro_command --case_tag top_p_sampling_p09
top_p_sampling_from_probs,0.071664,0.00048469280534742286,0,0.22908149140433132,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128256,,0.95,,,,,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,top_p_sampling_p095,True,python3 flashinfer_benchmark.py --routine top_p_sampling_from_probs --batch_size 32 --vocab_size 128256 --top_p 0.95 -vv --generate_repro_command --case_tag top_p_sampling_p095
top_k_top_p_sampling_from_probs,0.043567999999999996,0.00013004975543571325,0,0.09401689313257439,cuda,,,,,,,,,,,,,,,,,,,,,,50,,,,,,,,,,,,,,,,,,,,,50,,,,,,,,,,,,32000,50,0.9,,,,top_k_first,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,top_k_top_p_probs,True,python3 flashinfer_benchmark.py --routine top_k_top_p_sampling_from_probs --batch_size 32 --vocab_size 32000 --top_k 50 --top_p 0.9 --filter_apply_order top_k_first -vv --generate_repro_command --case_tag top_k_top_p_probs
top_k_top_p_sampling_from_logits,0.050304,0.00016321227146129575,0,0.08142748091603054,cuda,,,,,,,,,,,,,,,,,,,,,,50,,,,,,,,,,,,,,,,,,,,,50,,,,,,,,,,,,32000,50,0.9,,,,top_k_first,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,top_k_top_p_logits,True,python3 flashinfer_benchmark.py --routine top_k_top_p_sampling_from_logits --batch_size 32 --vocab_size 32000 --top_k 50 --top_p 0.9 --filter_apply_order top_k_first --input_dtype float32 -vv --generate_repro_command --case_tag top_k_top_p_logits
min_p_sampling_from_probs,0.0127045,0.00010446553924088511,0,0.3224155220591129,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32000,,,0.1,,,,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,min_p_sampling_p01,True,python3 flashinfer_benchmark.py --routine min_p_sampling_from_probs --batch_size 32 --vocab_size 32000 --min_p 0.1 -vv --generate_repro_command --case_tag min_p_sampling_p01
min_p_sampling_from_probs,0.041808,0.00017156848001114136,0,0.392673555300421,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128256,,,0.05,,,,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,min_p_sampling_p005,True,python3 flashinfer_benchmark.py --routine min_p_sampling_from_probs --batch_size 32 --vocab_size 128256 --min_p 0.05 -vv --generate_repro_command --case_tag min_p_sampling_p005
top_k_renorm_probs,0.023552,0.00016119284516793347,0,0.34782608695652173,cuda,,,,,,,,,,,,,,,,,,,,,,50,,,,,,,,,,,,,,,,,,,,,50,,,,,,,,,,,,32000,50,,,,,,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,top_k_renorm,True,python3 flashinfer_benchmark.py --routine top_k_renorm_probs --batch_size 32 --vocab_size 32000 --top_k 50 --input_dtype float32 -vv --generate_repro_command --case_tag top_k_renorm
top_p_renorm_probs,0.08048,0.00015717873760644349,0,0.10178926441351889,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32000,,0.9,,,,,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,top_p_renorm,True,python3 flashinfer_benchmark.py --routine top_p_renorm_probs --batch_size 32 --vocab_size 32000 --top_p 0.9 -vv --generate_repro_command --case_tag top_p_renorm
top_k_mask_logits,0.020448,0.00010783782061760889,0,0.40062597809076683,cuda,,,,,,,,,,,,,,,,,,,,,,50,,,,,,,,,,,,,,,,,,,,,50,,,,,,,,,,,,32000,50,,,,,,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,top_k_mask,True,python3 flashinfer_benchmark.py --routine top_k_mask_logits --batch_size 32 --vocab_size 32000 --top_k 50 --input_dtype float32 -vv --generate_repro_command --case_tag top_k_mask
chain_speculative_sampling,0.027168,0.00045243810626427136,0,0.8292414605418139,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32000,,,,,5,,,,,,,,,,,,16,,float32,,,,,,,,,,False,False,True,False,42,chain_spec_sampling_5,True,python3 flashinfer_benchmark.py --routine chain_speculative_sampling --batch_size 16 --vocab_size 32000 --num_speculate_tokens 5 -vv --generate_repro_command --case_tag chain_spec_sampling_5
chain_speculative_sampling,0.078304,0.00041787000237979534,0,3.564153657539845,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128256,,,,,8,,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,chain_spec_sampling_8,True,python3 flashinfer_benchmark.py --routine chain_speculative_sampling --batch_size 32 --vocab_size 128256 --num_speculate_tokens 8 -vv --generate_repro_command --case_tag chain_spec_sampling_8
top_k,0.017551999999999998,0.00014782604266126072,0,0.23445761166818596,cuda,,,,,,,,,,,,,,,,,,,,,,50,,,,,,,,,,,,,,,,,,,,,50,,,,,,,,,,,,32000,50,,,,,,,,,,,,,,,,32,,float32,,,,,,,,,,False,False,True,False,42,top_k_radix,True,python3 flashinfer_benchmark.py --routine top_k --batch_size 32 --vocab_size 32000 --top_k 50 --input_dtype float32 -vv --generate_repro_command --case_tag top_k_radix
top_k,0.038896,0.00019824246938198372,0,0.4237136980666392,cuda,,,,,,,,,,,,,,,,,,,,,,100,,,,,,,,,,,,,,,,,,,,,100,,,,,,,,,,,,128256,100,,,,,,,,,,,,,,,,64,,bfloat16,,,,,,,,,,False,False,True,False,42,top_k_radix_large,True,python3 flashinfer_benchmark.py --routine top_k --batch_size 64 --vocab_size 128256 --top_k 100 --input_dtype bfloat16 -vv --generate_repro_command --case_tag top_k_radix_large
top_k_page_table_transform,0.0080005,0.00012896701128583244,0,0.06604387225798387,cuda,,,,,,,,,,,,,,,,,,,,,,64,,,,,,,,,,,,,,,,,,,,,64,,,,,,,,,,,,,64,,,,,,4096,16,,,,,,,,,16,,float32,,,,,,,,,,False,False,True,False,42,top_k_page_table,True,python3 flashinfer_benchmark.py --routine top_k_page_table_transform --batch_size 16 --num_rows 16 --max_len 4096 --top_k 64 --input_dtype float32 -vv --generate_repro_command --case_tag top_k_page_table
top_k_ragged_transform,0.007424,0.00011858433941948468,0,0.03586206896551724,cuda,,,,,,,,,,,,,,,,,,,,,,64,,,,,,,,,,,,,,,,,,,,,64,,,,,,,,,,,,,64,,,,,,4096,16,,,,,,,,,16,,float32,,,,,,,,,,False,False,True,False,42,top_k_ragged,True,python3 flashinfer_benchmark.py --routine top_k_ragged_transform --batch_size 16 --num_rows 16 --max_len 4096 --top_k 64 --input_dtype float32 -vv --generate_repro_command --case_tag top_k_ragged
apply_rope,0.122912,0.0006609962724722589,0,2.729955740692528,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1024,128,128,,10000.0,1.0,False,,16,,float16,,fp8_e4m3,,,32,8,16,,,False,False,True,False,42,apply_rope_llama,True,python3 flashinfer_benchmark.py --routine apply_rope --batch_size 16 --seq_len 1024 --num_qo_heads 32 --num_kv_heads 8 --head_dim 128 --input_dtype float16 -vv --generate_repro_command --case_tag apply_rope_llama
apply_rope,0.5599689999999999,0.0022532907592131843,0,4.314380088897779,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2048,128,128,,10000.0,1.0,False,,32,,bfloat16,,fp8_e4m3,,,64,8,16,,,False,False,True,False,42,apply_rope_llama70b,True,python3 flashinfer_benchmark.py --routine apply_rope --batch_size 32 --seq_len 2048 --num_qo_heads 64 --num_kv_heads 8 --head_dim 128 --input_dtype bfloat16 -vv --generate_repro_command --case_tag apply_rope_llama70b
apply_rope_pos_ids,0.0809755,0.00050208063650728,0,4.144585164648567,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1024,128,128,,10000.0,1.0,False,,16,,float16,,fp8_e4m3,,,32,8,16,,,False,False,True,False,42,apply_rope_pos_ids,True,python3 flashinfer_benchmark.py --routine apply_rope_pos_ids --batch_size 16 --seq_len 1024 --num_qo_heads 32 --num_kv_heads 8 --head_dim 128 --input_dtype float16 -vv --generate_repro_command --case_tag apply_rope_pos_ids
apply_rope_pos_ids,0.41246499999999997,0.0005260596924304349,0,5.857906120519317,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2048,128,128,,10000.0,1.0,True,,32,,bfloat16,,fp8_e4m3,,,64,8,16,,,False,False,True,False,42,apply_rope_pos_ids_interleave,True,python3 flashinfer_benchmark.py --routine apply_rope_pos_ids --batch_size 32 --seq_len 2048 --num_qo_heads 64 --num_kv_heads 8 --head_dim 128 --input_dtype bfloat16 --interleave -vv --generate_repro_command --case_tag apply_rope_pos_ids_interleave
apply_llama31_rope,0.1242245,0.0004841010695666286,0,2.701112260463918,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1024,128,128,,500000.0,1.0,False,,16,,bfloat16,,fp8_e4m3,,,32,8,16,,,False,False,True,False,42,apply_llama31_rope,True,python3 flashinfer_benchmark.py --routine apply_llama31_rope --batch_size 16 --seq_len 1024 --num_qo_heads 32 --num_kv_heads 8 --head_dim 128 --rope_theta 500000.0 --rope_scale 1.0 --low_freq_factor 1.0 --high_freq_factor 4.0 --old_context_len 8192 --input_dtype bfloat16 -vv --generate_repro_command --case_tag apply_llama31_rope
apply_llama31_rope_pos_ids,0.081233,0.0005974024662375163,0,4.131447268967046,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1024,128,128,,500000.0,1.0,False,,16,,bfloat16,,fp8_e4m3,,,32,8,16,,,False,False,True,False,42,apply_llama31_rope_pos_ids,True,python3 flashinfer_benchmark.py --routine apply_llama31_rope_pos_ids --batch_size 16 --seq_len 1024 --num_qo_heads 32 --num_kv_heads 8 --head_dim 128 --rope_theta 500000.0 --rope_scale 1.0 --low_freq_factor 1.0 --high_freq_factor 4.0 --old_context_len 8192 --input_dtype bfloat16 -vv --generate_repro_command --case_tag apply_llama31_rope_pos_ids
mla_rope_quantize_fp8,0.441457,0.0014416343873072154,0,2.759015170220429,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1024,192,,64,,,False,,16,,bfloat16,,fp8_e4m3,,,128,128,16,,,False,False,True,False,42,mla_rope_fp8_deepseek,True,python3 flashinfer_benchmark.py --routine mla_rope_quantize_fp8 --batch_size 16 --seq_len 1024 --num_qo_heads 128 --num_kv_heads 128 --head_dim 192 --no_rope_dim 64 --input_dtype bfloat16 --quant_dtype fp8_e4m3 -vv --generate_repro_command --case_tag mla_rope_fp8_deepseek
rope_quantize_fp8,0.08296,0.0005573869352214454,0,3.040598649951784,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1024,128,128,0,,,False,,16,,bfloat16,,fp8_e4m3,,,32,8,16,,,False,False,True,False,42,rope_fp8_llama,True,python3 flashinfer_benchmark.py --routine rope_quantize_fp8 --batch_size 16 --seq_len 1024 --num_qo_heads 32 --num_kv_heads 8 --head_dim 128 --input_dtype bfloat16 --quant_dtype fp8_e4m3 -vv --generate_repro_command --case_tag rope_fp8_llama
rope_quantize_fp8,0.5697295,0.002435763801949799,0,3.182650798317447,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2048,128,128,0,,,False,,32,,bfloat16,,fp8_e4m3,,,64,8,16,,,False,False,True,False,42,rope_fp8_llama70b,True,python3 flashinfer_benchmark.py --routine rope_quantize_fp8 --batch_size 32 --seq_len 2048 --num_qo_heads 64 --num_kv_heads 8 --head_dim 128 --input_dtype bfloat16 --quant_dtype fp8_e4m3 -vv --generate_repro_command --case_tag rope_fp8_llama70b
rope_quantize_fp8_append_paged_kv_cache,0.0097445,0.00012860200879716725,0,1.9407082969880445,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,64,128,128,0,,,False,NHD,16,,bfloat16,,fp8_e4m3,,,32,8,16,,,False,False,True,False,42,rope_fp8_paged_kv,True,python3 flashinfer_benchmark.py --routine rope_quantize_fp8_append_paged_kv_cache --batch_size 16 --seq_len 64 --num_qo_heads 32 --num_kv_heads 8 --head_dim 128 --page_size 16 --kv_layout NHD --input_dtype bfloat16 --quant_dtype fp8_e4m3 -vv --generate_repro_command --case_tag rope_fp8_paged_kv
rope_quantize_fp8_append_paged_kv_cache,0.0262405,0.00016648466395837577,0,2.399173796231017,cuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,64,128,128,0,,,False,HND,32,,bfloat16,,fp8_e4m3,,,64,8,16,,,False,False,True,False,42,rope_fp8_paged_kv_hnd,True,python3 flashinfer_benchmark.py --routine rope_quantize_fp8_append_paged_kv_cache --batch_size 32 --seq_len 64 --num_qo_heads 64 --num_kv_heads 8 --head_dim 128 --page_size 16 --kv_layout HND --input_dtype bfloat16 --quant_dtype fp8_e4m3 -vv --generate_repro_command --case_tag rope_fp8_paged_kv_hnd
